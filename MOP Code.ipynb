{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a46bae57-eafd-4599-a192-c2924cdc3611",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, roc_curve, auc\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "817f02d2-a50d-466f-a70b-4847f8b47a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "TARGET_COLUMN = 'Opioid.Prescriber'\n",
    "CATEGORICAL_FEATURES = ['Gender', 'State', 'Credentials', 'Specialty']\n",
    "MODEL_FILENAME = 'finalized_model.sav'\n",
    "TEST_SIZE = 0.2\n",
    "RANDOM_STATE = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "7f792f7f-d04a-4b3a-af7e-51a0a1606c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_analyze_data():\n",
    "    \"\"\"\n",
    "    Loads all relevant datasets and performs initial data analysis.\n",
    "    This function checks for file existence and provides a summary of each dataset.\n",
    "    \"\"\"\n",
    "    print(\"=====================================================\")\n",
    "    print(\"--- 1. Data Loading and Initial Analysis ---\")\n",
    "    print(\"=====================================================\")\n",
    "    \n",
    "    try:\n",
    "        # Load the primary dataset for training\n",
    "        df = pd.read_csv('Dataset/prescriber-info.csv')\n",
    "        \n",
    "        # Load supplementary datasets for analysis and potential feature engineering\n",
    "        overdoses_df = pd.read_csv('Dataset/overdoses.csv')\n",
    "        Dataset_Upload_df = pd.read_csv('Dataset/Dataset_Upload.csv')\n",
    "        opioids_df = pd.read_csv('Dataset/opioids.csv')\n",
    "        \n",
    "        print(\"\\nSuccessfully loaded the following files:\")\n",
    "        print(f\"- prescriber-info.csv (Primary dataset, shape: {df.shape})\")\n",
    "        print(f\"- overdoses.csv (Supplementary data, shape: {overdoses_df.shape})\")\n",
    "        print(f\"- Dataset_Upload.csv (Duplicate of primary data, shape: {Dataset_Upload_df.shape})\")\n",
    "        print(f\"- opioids.csv (Supplementary data, shape: {opioids_df.shape})\")\n",
    "\n",
    "        print(\"\\n------------------ prescriber-info.csv Info ------------------\")\n",
    "        print(df.info())\n",
    "        print(\"\\nFirst 5 rows:\")\n",
    "        print(df.head())\n",
    "        print(\"\\nMissing values:\")\n",
    "        print(df.isnull().sum())\n",
    "        \n",
    "        print(\"\\n------------------ overdoses.csv Info ------------------\")\n",
    "        print(overdoses_df.info())\n",
    "        print(\"\\nFirst 5 rows:\")\n",
    "        print(overdoses_df.head())\n",
    "\n",
    "        print(\"\\n------------------ Dataset_Upload.csv Info ------------------\")\n",
    "        print(Dataset_Upload_df.info())\n",
    "        print(\"\\nFirst 5 rows:\")\n",
    "        print(Dataset_Upload_df.head())\n",
    "\n",
    "        print(\"\\n------------------ opioids.csv Info ------------------\")\n",
    "        print(opioids_df.info())\n",
    "        print(\"\\nFirst 5 rows:\")\n",
    "        print(opioids_df.head())\n",
    "\n",
    "        print(\"\\n--- Initial analysis complete. Proceeding to preprocessing. ---\\n\")\n",
    "        return df, overdoses_df, Dataset_Upload_df, opioids_df\n",
    "    \n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error: {e}. Please ensure all necessary CSV files are available in the directory.\")\n",
    "        return None, None, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "fc15af19-4f86-46ae-8c5e-5a664a5e96a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(df):\n",
    "    \"\"\"\n",
    "    Performs data cleaning, feature engineering, and encoding on the main dataframe.\n",
    "    \"\"\"\n",
    "    print(\"=====================================================\")\n",
    "    print(\"--- 2. Data Preprocessing and Feature Engineering ---\")\n",
    "    print(\"=====================================================\")\n",
    "    \n",
    "    # Create a copy to avoid SettingWithCopyWarning\n",
    "    df_processed = df.copy()\n",
    "\n",
    "    # Drop the 'Id' column as it is an identifier and not a predictive feature\n",
    "    if 'Id' in df_processed.columns:\n",
    "        print(\"Dropping 'Id' column.\")\n",
    "        df_processed = df_processed.drop('Id', axis=1)\n",
    "\n",
    "    # Clean and encode categorical features\n",
    "    print(\"Processing categorical features...\")\n",
    "    for feature in CATEGORICAL_FEATURES:\n",
    "        if feature in df_processed.columns:\n",
    "            le = LabelEncoder()\n",
    "            # Standardize string data (uppercase, remove spaces and periods)\n",
    "            df_processed[feature] = df_processed[feature].astype(str).str.upper().str.strip().str.replace('.', '').str.replace(' ', '')\n",
    "            \n",
    "            # Check for unique values before encoding to handle new categories gracefully\n",
    "            unique_values = df_processed[feature].nunique()\n",
    "            print(f\"Feature '{feature}' has {unique_values} unique values before encoding.\")\n",
    "            \n",
    "            # Fit and transform the feature\n",
    "            df_processed[feature] = le.fit_transform(df_processed[feature])\n",
    "            print(f\"Feature '{feature}' encoded. Example values: {df_processed[feature].head().tolist()}\")\n",
    "            \n",
    "    # Analyzing the distribution of the target variable\n",
    "    print(\"\\nAnalyzing target variable distribution:\")\n",
    "    target_counts = df_processed[TARGET_COLUMN].value_counts()\n",
    "    print(target_counts)\n",
    "    print(f\"Class 0 (Non-Opioid Prescriber): {target_counts.get(0, 0)/len(df_processed)*100:.2f}%\")\n",
    "    print(f\"Class 1 (Opioid Prescriber): {target_counts.get(1, 0)/len(df_processed)*100:.2f}%\")\n",
    "    \n",
    "    print(\"\\n--- Preprocessing and feature engineering complete. ---\\n\")\n",
    "    return df_processed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "3c991c6a-7289-43ae-b2b2-6f731351a1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(df_processed):\n",
    "    \"\"\"\n",
    "    Splits the preprocessed dataframe into training and testing sets.\n",
    "    \"\"\"\n",
    "    print(\"=====================================================\")\n",
    "    print(\"--- 3. Data Splitting ---\")\n",
    "    print(\"=====================================================\")\n",
    "    \n",
    "    # Separate features and target variable\n",
    "    if TARGET_COLUMN not in df_processed.columns:\n",
    "        print(f\"Error: Target column '{TARGET_COLUMN}' not found in the dataframe.\")\n",
    "        return None, None, None, None\n",
    "        \n",
    "    X = df_processed.drop(TARGET_COLUMN, axis=1)\n",
    "    y = df_processed[TARGET_COLUMN]\n",
    "\n",
    "    # Split the data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=y)\n",
    "    \n",
    "    print(f\"Total dataset size: {len(df_processed)} samples\")\n",
    "    print(f\"Training set size: {X_train.shape[0]} samples\")\n",
    "    print(f\"Testing set size: {X_test.shape[0]} samples\")\n",
    "    \n",
    "    print(\"\\n--- Data split complete. ---\\n\")\n",
    "    return X_train, X_test, y_train, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "15782d7c-2bd3-4182-80cb-03c7461a09af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(X_train, y_train):\n",
    "    print(\"=====================================================\")\n",
    "    print(\"--- 4. Model Training ---\")\n",
    "    print(\"=====================================================\")\n",
    "    \n",
    "    # Model initialization with hyperparameters\n",
    "    print(\"Initializing Classifier...\")\n",
    "    model = RandomForestClassifier(\n",
    "        n_estimators=100,      # Number of trees in the forest\n",
    "        criterion='gini',      # The function to measure the quality of a split\n",
    "        max_depth=None,        # Maximum number of levels in tree\n",
    "        min_samples_split=2,   # Minimum number of data points placed in a node before the node is split\n",
    "        min_samples_leaf=1,    # Minimum number of data points allowed in a leaf node\n",
    "        random_state=RANDOM_STATE,\n",
    "        n_jobs=-1              # Use all available CPU cores for training\n",
    "    )\n",
    "    \n",
    "    print(\"Training the model on the training data...\")\n",
    "    model.fit(X_train, y_train)\n",
    "    print(\"Model training complete.\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a4c2f013-17fb-4448-b649-dbf88beb35fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_and_save_model(model, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Evaluates the trained model and saves it to a file.\n",
    "    \"\"\"\n",
    "    print(\"=====================================================\")\n",
    "    print(\"--- 5. Model Evaluation and Saving ---\")\n",
    "    print(\"=====================================================\")\n",
    "    \n",
    "    # Make predictions on the test set\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_proba = model.predict_proba(X_test)\n",
    "    \n",
    "    # Detailed evaluation metrics\n",
    "    print(\"\\n--- Detailed Model Evaluation ---\\n\")\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    \n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1-Score: {f1:.4f}\")\n",
    "    \n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    print(cm)\n",
    "    \n",
    "    # --- Feature Importance Analysis ---\n",
    "    print(\"\\n--- Feature Importance Analysis ---\\n\")\n",
    "    importances = model.feature_importances_\n",
    "    feature_names = X_test.columns\n",
    "    feature_importance_df = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'Importance': importances\n",
    "    }).sort_values(by='Importance', ascending=False)\n",
    "    \n",
    "    print(feature_importance_df)\n",
    "    \n",
    "    # --- Saving the trained model ---\n",
    "    print(\"\\nSaving the trained model...\")\n",
    "    with open(MODEL_FILENAME, 'wb') as file:\n",
    "        pickle.dump(model, file)\n",
    "    print(f\"Model successfully saved to '{MODEL_FILENAME}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f24233d-d3dc-43b3-8d50-a61a814a1739",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_results(model, X_test, y_test, df_processed):\n",
    "    \"\"\"\n",
    "    Generates visualizations for the model's performance and feature importances.\n",
    "    \"\"\"\n",
    "    print(\"=====================================================\")\n",
    "    print(\"--- 6. Visualizing Results ---\")\n",
    "    print(\"=====================================================\")\n",
    "    \n",
    "    # Feature Importance Plot\n",
    "    importances = model.feature_importances_\n",
    "    feature_names = X_test.columns\n",
    "    sorted_indices = np.argsort(importances)[::-1]\n",
    "    \n",
    "    plt.figure(figsize=(15, 8))\n",
    "    plt.title(\"Feature Importance\")\n",
    "    plt.bar(range(X_test.shape[1]), importances[sorted_indices], align='center')\n",
    "    plt.xticks(range(X_test.shape[1]), feature_names[sorted_indices], rotation='vertical')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"Feature_Importance_Plot.png\")\n",
    "    plt.close()\n",
    "\n",
    "     # -------- 1. Class Distribution Plot --------\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.countplot(x='Opioid.Prescriber', data=df_processed)\n",
    "    plt.title(\"Class Distribution (Opioid Prescriber vs Non-Prescriber)\")\n",
    "    plt.xlabel(\"Opioid Prescriber\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.savefig(\"class_distribution.png\")\n",
    "    plt.close()\n",
    "\n",
    "    # -------- 2. Feature Importance Plot --------\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'Feature': X_test.columns,\n",
    "        'Importance': model.feature_importances_\n",
    "    }).sort_values(by=\"Importance\", ascending=False)\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(x=\"Importance\", y=\"Feature\", data=feature_importance.head(15))\n",
    "    plt.title(\"Top 15 Feature Importances (Random Forest)\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"feature_importance.png\")\n",
    "    plt.close()\n",
    "\n",
    "    # -------- 3. Confusion Matrix --------\n",
    "    y_pred = model.predict(X_test)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"Actual\")\n",
    "    plt.savefig(\"confusion_matrix.png\")\n",
    "    plt.close()\n",
    "\n",
    "    # -------- 4. ROC Curve (Optional) --------\n",
    "    if hasattr(model, \"predict_proba\"):\n",
    "        y_proba = model.predict_proba(X_test)[:, 1]\n",
    "        fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "\n",
    "        plt.figure(figsize=(6, 5))\n",
    "        plt.plot(fpr, tpr, label=f'ROC Curve (AUC = {roc_auc:.2f})')\n",
    "        plt.plot([0, 1], [0, 1], 'k--')\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title('ROC Curve')\n",
    "        plt.legend(loc='lower right')\n",
    "        plt.savefig(\"roc_curve.png\")\n",
    "        plt.close()\n",
    "  \n",
    "        \n",
    "    print(\"✅ All visualization images saved: class_distribution.png, feature_importance.png, confusion_matrix.png, roc_curve.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "8744b926-40fa-45a3-a037-5a214ba4b8a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================================\n",
      "--- 1. Data Loading and Initial Analysis ---\n",
      "=====================================================\n",
      "\n",
      "Successfully loaded the following files:\n",
      "- prescriber-info.csv (Primary dataset, shape: (25000, 256))\n",
      "- overdoses.csv (Supplementary data, shape: (50, 4))\n",
      "- Dataset_Upload.csv (Duplicate of primary data, shape: (352, 256))\n",
      "- opioids.csv (Supplementary data, shape: (113, 2))\n",
      "\n",
      "------------------ prescriber-info.csv Info ------------------\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 25000 entries, 0 to 24999\n",
      "Columns: 256 entries, Id to Opioid.Prescriber\n",
      "dtypes: int64(252), object(4)\n",
      "memory usage: 48.8+ MB\n",
      "None\n",
      "\n",
      "First 5 rows:\n",
      "           Id Gender State Credentials            Specialty  ABILIFY  \\\n",
      "0  1710982582      M    TX         DDS              Dentist        0   \n",
      "1  1245278100      F    AL          MD      General Surgery        0   \n",
      "2  1427182161      F    NY        M.D.     General Practice        0   \n",
      "3  1669567541      M    AZ          MD    Internal Medicine        0   \n",
      "4  1679650949      M    NV        M.D.  Hematology/Oncology        0   \n",
      "\n",
      "   ACETAMINOPHEN.CODEINE  ACYCLOVIR  ADVAIR.DISKUS  AGGRENOX  ...  \\\n",
      "0                      0          0              0         0  ...   \n",
      "1                      0          0              0         0  ...   \n",
      "2                      0          0              0         0  ...   \n",
      "3                     43          0              0         0  ...   \n",
      "4                      0          0              0         0  ...   \n",
      "\n",
      "   VERAPAMIL.ER  VESICARE  VOLTAREN  VYTORIN  WARFARIN.SODIUM  XARELTO  ZETIA  \\\n",
      "0             0         0         0        0                0        0      0   \n",
      "1             0         0         0        0                0        0      0   \n",
      "2             0         0         0        0                0        0      0   \n",
      "3             0         0         0        0                0        0      0   \n",
      "4             0         0         0        0               17       28      0   \n",
      "\n",
      "   ZIPRASIDONE.HCL  ZOLPIDEM.TARTRATE  Opioid.Prescriber  \n",
      "0                0                  0                  1  \n",
      "1                0                 35                  1  \n",
      "2                0                 25                  0  \n",
      "3                0                  0                  1  \n",
      "4                0                  0                  1  \n",
      "\n",
      "[5 rows x 256 columns]\n",
      "\n",
      "Missing values:\n",
      "Id                     0\n",
      "Gender                 0\n",
      "State                  0\n",
      "Credentials          763\n",
      "Specialty              0\n",
      "                    ... \n",
      "XARELTO                0\n",
      "ZETIA                  0\n",
      "ZIPRASIDONE.HCL        0\n",
      "ZOLPIDEM.TARTRATE      0\n",
      "Opioid.Prescriber      0\n",
      "Length: 256, dtype: int64\n",
      "\n",
      "------------------ overdoses.csv Info ------------------\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 50 entries, 0 to 49\n",
      "Data columns (total 4 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   State       50 non-null     object\n",
      " 1   Population  50 non-null     object\n",
      " 2   Deaths      50 non-null     object\n",
      " 3   Abbrev      50 non-null     object\n",
      "dtypes: object(4)\n",
      "memory usage: 1.7+ KB\n",
      "None\n",
      "\n",
      "First 5 rows:\n",
      "        State  Population Deaths Abbrev\n",
      "0     Alabama   4,833,722    723     AL\n",
      "1      Alaska     735,132    124     AK\n",
      "2     Arizona   6,626,624  1,211     AZ\n",
      "3    Arkansas   2,959,373    356     AR\n",
      "4  California  38,332,521  4,521     CA\n",
      "\n",
      "------------------ Dataset_Upload.csv Info ------------------\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 352 entries, 0 to 351\n",
      "Columns: 256 entries, Id to Opioid.Prescriber\n",
      "dtypes: int64(252), object(4)\n",
      "memory usage: 704.1+ KB\n",
      "None\n",
      "\n",
      "First 5 rows:\n",
      "           Id Gender State Credentials            Specialty  ABILIFY  \\\n",
      "0  1710982582      M    TX         DDS              Dentist        0   \n",
      "1  1245278100      F    AL          MD      General Surgery        0   \n",
      "2  1427182161      F    NY        M.D.     General Practice        0   \n",
      "3  1669567541      M    AZ          MD    Internal Medicine        0   \n",
      "4  1679650949      M    NV        M.D.  Hematology/Oncology        0   \n",
      "\n",
      "   ACETAMINOPHEN.CODEINE  ACYCLOVIR  ADVAIR.DISKUS  AGGRENOX  ...  \\\n",
      "0                      0          0              0         0  ...   \n",
      "1                      0          0              0         0  ...   \n",
      "2                      0          0              0         0  ...   \n",
      "3                     43          0              0         0  ...   \n",
      "4                      0          0              0         0  ...   \n",
      "\n",
      "   VERAPAMIL.ER  VESICARE  VOLTAREN  VYTORIN  WARFARIN.SODIUM  XARELTO  ZETIA  \\\n",
      "0             0         0         0        0                0        0      0   \n",
      "1             0         0         0        0                0        0      0   \n",
      "2             0         0         0        0                0        0      0   \n",
      "3             0         0         0        0                0        0      0   \n",
      "4             0         0         0        0               17       28      0   \n",
      "\n",
      "   ZIPRASIDONE.HCL  ZOLPIDEM.TARTRATE  Opioid.Prescriber  \n",
      "0                0                  0                  1  \n",
      "1                0                 35                  1  \n",
      "2                0                 25                  0  \n",
      "3                0                  0                  1  \n",
      "4                0                  0                  1  \n",
      "\n",
      "[5 rows x 256 columns]\n",
      "\n",
      "------------------ opioids.csv Info ------------------\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 113 entries, 0 to 112\n",
      "Data columns (total 2 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   Drug Name     113 non-null    object\n",
      " 1   Generic Name  113 non-null    object\n",
      "dtypes: object(2)\n",
      "memory usage: 1.9+ KB\n",
      "None\n",
      "\n",
      "First 5 rows:\n",
      "                        Drug Name                    Generic Name\n",
      "0                         ABSTRAL                FENTANYL CITRATE\n",
      "1           ACETAMINOPHEN-CODEINE      ACETAMINOPHEN WITH CODEINE\n",
      "2                           ACTIQ                FENTANYL CITRATE\n",
      "3             ASCOMP WITH CODEINE  CODEINE/BUTALBITAL/ASA/CAFFEIN\n",
      "4  ASPIRIN-CAFFEINE-DIHYDROCODEIN  DIHYDROCODEINE/ASPIRIN/CAFFEIN\n",
      "\n",
      "--- Initial analysis complete. Proceeding to preprocessing. ---\n",
      "\n",
      "=====================================================\n",
      "--- 2. Data Preprocessing and Feature Engineering ---\n",
      "=====================================================\n",
      "Dropping 'Id' column.\n",
      "Processing categorical features...\n",
      "Feature 'Gender' has 2 unique values before encoding.\n",
      "Feature 'Gender' encoded. Example values: [1, 0, 0, 1, 1]\n",
      "Feature 'State' has 57 unique values before encoding.\n",
      "Feature 'State' encoded. Example values: [47, 3, 37, 5, 36]\n",
      "Feature 'Credentials' has 665 unique values before encoding.\n",
      "Feature 'Credentials' encoded. Example values: [141, 304, 304, 304, 304]\n",
      "Feature 'Specialty' has 109 unique values before encoding.\n",
      "Feature 'Specialty' encoded. Example values: [18, 28, 27, 41, 35]\n",
      "\n",
      "Analyzing target variable distribution:\n",
      "Opioid.Prescriber\n",
      "1    14688\n",
      "0    10312\n",
      "Name: count, dtype: int64\n",
      "Class 0 (Non-Opioid Prescriber): 41.25%\n",
      "Class 1 (Opioid Prescriber): 58.75%\n",
      "\n",
      "--- Preprocessing and feature engineering complete. ---\n",
      "\n",
      "=====================================================\n",
      "--- 3. Data Splitting ---\n",
      "=====================================================\n",
      "Total dataset size: 25000 samples\n",
      "Training set size: 20000 samples\n",
      "Testing set size: 5000 samples\n",
      "\n",
      "--- Data split complete. ---\n",
      "\n",
      "=====================================================\n",
      "--- 4. Model Training ---\n",
      "=====================================================\n",
      "Initializing Classifier...\n",
      "Training the model on the training data...\n",
      "Model training complete.\n",
      "=====================================================\n",
      "--- 5. Model Evaluation and Saving ---\n",
      "=====================================================\n",
      "\n",
      "--- Detailed Model Evaluation ---\n",
      "\n",
      "Accuracy: 0.9210\n",
      "Precision: 0.9582\n",
      "Recall: 0.9050\n",
      "F1-Score: 0.9309\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.94      0.91      2062\n",
      "           1       0.96      0.91      0.93      2938\n",
      "\n",
      "    accuracy                           0.92      5000\n",
      "   macro avg       0.92      0.92      0.92      5000\n",
      "weighted avg       0.92      0.92      0.92      5000\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[1946  116]\n",
      " [ 279 2659]]\n",
      "\n",
      "--- Feature Importance Analysis ---\n",
      "\n",
      "                       Feature  Importance\n",
      "104  HYDROCODONE.ACETAMINOPHEN    0.248319\n",
      "185    OXYCODONE.ACETAMINOPHEN    0.075336\n",
      "234               TRAMADOL.HCL    0.065179\n",
      "186              OXYCODONE.HCL    0.031720\n",
      "1                        State    0.029484\n",
      "..                         ...         ...\n",
      "99                   GLYBURIDE    0.000099\n",
      "119               KLOR.CON.M10    0.000091\n",
      "112                 IRBESARTAN    0.000089\n",
      "181                    ONGLYZA    0.000072\n",
      "248                    VYTORIN    0.000065\n",
      "\n",
      "[254 rows x 2 columns]\n",
      "\n",
      "Saving the trained model...\n",
      "Model successfully saved to 'finalized_model.sav'\n",
      "=====================================================\n",
      "--- 6. Visualizing Results ---\n",
      "=====================================================\n",
      "✅ All visualization images saved: class_distribution.png, feature_importance.png, confusion_matrix.png, roc_curve.png\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to orchestrate the entire machine learning pipeline.\n",
    "    \"\"\"\n",
    "    df, _, _, _ = load_and_analyze_data()\n",
    "    \n",
    "    if df is not None:\n",
    "        df_processed = preprocess_data(df)\n",
    "        X_train, X_test, y_train, y_test = split_data(df_processed)\n",
    "        \n",
    "        if X_train is not None:\n",
    "            model = train_model(X_train, y_train)\n",
    "            evaluate_and_save_model(model, X_test, y_test)\n",
    "            # Uncomment the line below to generate plots after running the pipeline\n",
    "            visualize_results(model, X_test, y_test, df_processed)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8fd39d-63cc-4ec9-b0b3-4a3eeb1d982a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MedicineOverdoseprediction",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
